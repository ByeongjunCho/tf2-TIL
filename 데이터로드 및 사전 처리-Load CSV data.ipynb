{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv\n",
      "32768/30874 [===============================] - 0s 3us/step\n",
      "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/eval.csv\n",
      "16384/13049 [=====================================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\jjgk9\\\\.keras\\\\datasets\\\\train.csv',\n",
       " 'C:\\\\Users\\\\jjgk9\\\\.keras\\\\datasets\\\\eval.csv')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path, test_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy 형식을 읽기 편하게 설정\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "csv의 파일 형식을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone\n",
      "0,male,22.0,1,0,7.25,Third,unknown,Southampton,n\n",
      "1,female,38.0,1,0,71.2833,First,C,Cherbourg,n\n",
      "1,female,26.0,0,0,7.925,Third,unknown,Southampton,y\n",
      "1,female,35.0,1,0,53.1,First,C,Southampton,n\n",
      "0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y\n",
      "0,male,2.0,3,1,21.075,Third,unknown,Southampton,n\n",
      "1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n\n",
      "1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n\n",
      "1,female,4.0,1,1,16.7,Third,G,Southampton,n\n"
     ]
    }
   ],
   "source": [
    "! head {train_file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로, csv형식은 pandas로 load하고 `numpy`형식으로 `tensorflow`에 넘길 수 있다. 파일이 매우 크거나 tensorflow/tf.data를 이용해 사용하기 위해서는 `tf.data.experimental.make_csv_dataset` 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = 'survived'\n",
    "LABELS = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jjgk9\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\experimental\\ops\\readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(file_path, **kwargs):\n",
    "  dataset = tf.data.experimental.make_csv_dataset(\n",
    "      file_path,\n",
    "      batch_size=5, # 보이는 크기를 작게 설정\n",
    "      label_name=LABEL_COLUMN,\n",
    "      na_value=\"?\",\n",
    "      num_epochs=1,\n",
    "      ignore_errors=True, \n",
    "      **kwargs)\n",
    "  return dataset\n",
    "\n",
    "raw_train_data = get_dataset(train_file_path)\n",
    "raw_test_data = get_dataset(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (OrderedDict([(sex, (None,)), (age, (None,)), (n_siblings_spouses, (None,)), (parch, (None,)), (fare, (None,)), (class, (None,)), (deck, (None,)), (embark_town, (None,)), (alone, (None,))]), (None,)), types: (OrderedDict([(sex, tf.string), (age, tf.float32), (n_siblings_spouses, tf.int32), (parch, tf.int32), (fare, tf.float32), (class, tf.string), (deck, tf.string), (embark_town, tf.string), (alone, tf.string)]), tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "  for batch, label in dataset.take(1):\n",
    "#     print(\"batch : \", batch)\n",
    "#     print(\"label : \", label)\n",
    "    for key, value in batch.items():\n",
    "      print(\"{:20s}: {}\".format(key,value.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :  OrderedDict([('sex', <tf.Tensor: id=267, shape=(5,), dtype=string, numpy=array([b'male', b'male', b'male', b'male', b'male'], dtype=object)>), ('age', <tf.Tensor: id=259, shape=(5,), dtype=float32, numpy=array([21., 37., 36., 35., 28.], dtype=float32)>), ('n_siblings_spouses', <tf.Tensor: id=265, shape=(5,), dtype=int32, numpy=array([0, 2, 1, 0, 0])>), ('parch', <tf.Tensor: id=266, shape=(5,), dtype=int32, numpy=array([0, 0, 0, 0, 0])>), ('fare', <tf.Tensor: id=264, shape=(5,), dtype=float32, numpy=array([ 7.796,  7.925, 15.55 , 26.   ,  7.75 ], dtype=float32)>), ('class', <tf.Tensor: id=261, shape=(5,), dtype=string, numpy=array([b'Third', b'Third', b'Third', b'Second', b'Third'], dtype=object)>), ('deck', <tf.Tensor: id=262, shape=(5,), dtype=string, numpy=array([b'unknown', b'unknown', b'unknown', b'unknown', b'F'], dtype=object)>), ('embark_town', <tf.Tensor: id=263, shape=(5,), dtype=string, numpy=\n",
      "array([b'Southampton', b'Southampton', b'Southampton', b'Southampton',\n",
      "       b'Queenstown'], dtype=object)>), ('alone', <tf.Tensor: id=260, shape=(5,), dtype=string, numpy=array([b'y', b'n', b'n', b'y', b'y'], dtype=object)>)])\n",
      "label :  tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int32)\n",
      "sex                 : [b'male' b'male' b'male' b'male' b'male']\n",
      "age                 : [21. 37. 36. 35. 28.]\n",
      "n_siblings_spouses  : [0 2 1 0 0]\n",
      "parch               : [0 0 0 0 0]\n",
      "fare                : [ 7.796  7.925 15.55  26.     7.75 ]\n",
      "class               : [b'Third' b'Third' b'Third' b'Second' b'Third']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'unknown' b'F']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton' b'Southampton' b'Queenstown']\n",
      "alone               : [b'y' b'n' b'n' b'y' b'y']\n"
     ]
    }
   ],
   "source": [
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "열이 포함된 데이터의 경우 자동으로 열 이름을 지정함. 열 이름이 없는 데이터셋이라면, 직접 열 이름을 지정하고 데이터를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'male' b'male' b'male' b'male']\n",
      "age                 : [35. 19. 36. 28. 20.]\n",
      "n_siblings_spouses  : [0 3 0 0 0]\n",
      "parch               : [0 2 0 0 0]\n",
      "fare                : [ 26.288 263.     12.875  30.5     8.05 ]\n",
      "class               : [b'First' b'First' b'Second' b'First' b'Third']\n",
      "deck                : [b'E' b'C' b'D' b'C' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Cherbourg' b'Southampton' b'Southampton']\n",
      "alone               : [b'y' b'n' b'y' b'y' b'y']\n"
     ]
    }
   ],
   "source": [
    "CSV_COLUMNS = ['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare', 'class', 'deck', 'embark_town', 'alone']\n",
    "\n",
    "temp_dataset = get_dataset(train_file_path, column_names=CSV_COLUMNS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
