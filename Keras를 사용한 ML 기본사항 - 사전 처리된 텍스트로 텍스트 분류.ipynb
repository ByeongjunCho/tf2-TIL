{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 노트북은 영화 리뷰(review) 텍스트를 긍정(positive) 또는 부정(negative)으로 분류합니다. 이 예제는 이진(binary)-또는 클래스(class)가 두 개인- 분류 문제입니다. 이진 분류는 머신러닝에서 중요하고 널리 사용됩니다.\n",
    "\n",
    "이 튜토리얼에서는 텐서플로 허브(TensorFlow Hub)와 케라스(Keras)를 사용한 기초적인 전이 학습(transfer learning) 애플리케이션을 보여줍니다.\n",
    "\n",
    "여기에서는 인터넷 영화 데이터베이스(Internet Movie Database)에서 수집한 50,000개의 영화 리뷰 텍스트를 담은 IMDB 데이터셋을 사용하겠습니다. 25,000개 리뷰는 훈련용으로, 25,000개는 테스트용으로 나뉘어져 있습니다. 훈련 세트와 테스트 세트의 클래스는 균형이 잡혀 있습니다. 즉 긍정적인 리뷰와 부정적인 리뷰의 개수가 동일합니다.\n",
    "\n",
    "이 노트북은 텐서플로에서 모델을 만들고 훈련하기 위한 고수준 파이썬 API인 tf.keras와 전이 학습 라이브러리이자 플랫폼인 텐서플로 허브를 사용합니다. tf.keras를 사용한 고급 텍스트 분류 튜토리얼은 MLCC 텍스트 분류 가이드를 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - ETA:  - ETA: 1: - ETA: 59s - ETA: 51 - ETA: 45 - ETA: 34 - ETA: 27 - ETA: 23 - ETA: 20 - ETA: 15 - ETA: 12 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매개변수 `num_words=10000`은 훈련 데이터에서 가장 많이 등장하는 상위 10,000개의 단어를 선택합니다. 데이터 크기를 적당하게 유지하기 위해 드물에 등장하는 단어는 제외하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플: 25000, 레이블: 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 샘플: {}, 레이블: {}\".format(len(train_data), len(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 189)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정수를 단어로 다시 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fawn': 34701,\n",
       " 'tsukino': 52006,\n",
       " 'nunnery': 52007,\n",
       " 'sonja': 16816,\n",
       " 'vani': 63951,\n",
       " 'woods': 1408,\n",
       " 'spiders': 16115,\n",
       " 'hanging': 2345,\n",
       " 'woody': 2289,\n",
       " 'trawling': 52008,\n",
       " \"hold's\": 52009,\n",
       " 'comically': 11307,\n",
       " 'localized': 40830,\n",
       " 'disobeying': 30568,\n",
       " \"'royale\": 52010,\n",
       " \"harpo's\": 40831,\n",
       " 'canet': 52011,\n",
       " 'aileen': 19313,\n",
       " 'acurately': 52012,\n",
       " \"diplomat's\": 52013,\n",
       " 'rickman': 25242,\n",
       " 'arranged': 6746,\n",
       " 'rumbustious': 52014,\n",
       " 'familiarness': 52015,\n",
       " \"spider'\": 52016,\n",
       " 'hahahah': 68804,\n",
       " \"wood'\": 52017,\n",
       " 'transvestism': 40833,\n",
       " \"hangin'\": 34702,\n",
       " 'bringing': 2338,\n",
       " 'seamier': 40834,\n",
       " 'wooded': 34703,\n",
       " 'bravora': 52018,\n",
       " 'grueling': 16817,\n",
       " 'wooden': 1636,\n",
       " 'wednesday': 16818,\n",
       " \"'prix\": 52019,\n",
       " 'altagracia': 34704,\n",
       " 'circuitry': 52020,\n",
       " 'crotch': 11585,\n",
       " 'busybody': 57766,\n",
       " \"tart'n'tangy\": 52021,\n",
       " 'burgade': 14129,\n",
       " 'thrace': 52023,\n",
       " \"tom's\": 11038,\n",
       " 'snuggles': 52025,\n",
       " 'francesco': 29114,\n",
       " 'complainers': 52027,\n",
       " 'templarios': 52125,\n",
       " '272': 40835,\n",
       " '273': 52028,\n",
       " 'zaniacs': 52130,\n",
       " '275': 34706,\n",
       " 'consenting': 27631,\n",
       " 'snuggled': 40836,\n",
       " 'inanimate': 15492,\n",
       " 'uality': 52030,\n",
       " 'bronte': 11926,\n",
       " 'errors': 4010,\n",
       " 'dialogs': 3230,\n",
       " \"yomada's\": 52031,\n",
       " \"madman's\": 34707,\n",
       " 'dialoge': 30585,\n",
       " 'usenet': 52033,\n",
       " 'videodrome': 40837,\n",
       " \"kid'\": 26338,\n",
       " 'pawed': 52034,\n",
       " \"'girlfriend'\": 30569,\n",
       " \"'pleasure\": 52035,\n",
       " \"'reloaded'\": 52036,\n",
       " \"kazakos'\": 40839,\n",
       " 'rocque': 52037,\n",
       " 'mailings': 52038,\n",
       " 'brainwashed': 11927,\n",
       " 'mcanally': 16819,\n",
       " \"tom''\": 52039,\n",
       " 'kurupt': 25243,\n",
       " 'affiliated': 21905,\n",
       " 'babaganoosh': 52040,\n",
       " \"noe's\": 40840,\n",
       " 'quart': 40841,\n",
       " 'kids': 359,\n",
       " 'uplifting': 5034,\n",
       " 'controversy': 7093,\n",
       " 'kida': 21906,\n",
       " 'kidd': 23379,\n",
       " \"error'\": 52041,\n",
       " 'neurologist': 52042,\n",
       " 'spotty': 18510,\n",
       " 'cobblers': 30570,\n",
       " 'projection': 9878,\n",
       " 'fastforwarding': 40842,\n",
       " 'sters': 52043,\n",
       " \"eggar's\": 52044,\n",
       " 'etherything': 52045,\n",
       " 'gateshead': 40843,\n",
       " 'airball': 34708,\n",
       " 'unsinkable': 25244,\n",
       " 'stern': 7180,\n",
       " \"cervi's\": 52046,\n",
       " 'dnd': 40844,\n",
       " 'dna': 11586,\n",
       " 'insecurity': 20598,\n",
       " \"'reboot'\": 52047,\n",
       " 'trelkovsky': 11037,\n",
       " 'jaekel': 52048,\n",
       " 'sidebars': 52049,\n",
       " \"sforza's\": 52050,\n",
       " 'distortions': 17633,\n",
       " 'mutinies': 52051,\n",
       " 'sermons': 30602,\n",
       " '7ft': 40846,\n",
       " 'boobage': 52052,\n",
       " \"o'bannon's\": 52053,\n",
       " 'populations': 23380,\n",
       " 'chulak': 52054,\n",
       " 'mesmerize': 27633,\n",
       " 'quinnell': 52055,\n",
       " 'yahoo': 10307,\n",
       " 'meteorologist': 52057,\n",
       " 'beswick': 42577,\n",
       " 'boorman': 15493,\n",
       " 'voicework': 40847,\n",
       " \"ster'\": 52058,\n",
       " 'blustering': 22922,\n",
       " 'hj': 52059,\n",
       " 'intake': 27634,\n",
       " 'morally': 5621,\n",
       " 'jumbling': 40849,\n",
       " 'bowersock': 52060,\n",
       " \"'porky's'\": 52061,\n",
       " 'gershon': 16821,\n",
       " 'ludicrosity': 40850,\n",
       " 'coprophilia': 52062,\n",
       " 'expressively': 40851,\n",
       " \"india's\": 19500,\n",
       " \"post's\": 34710,\n",
       " 'wana': 52063,\n",
       " 'wang': 5283,\n",
       " 'wand': 30571,\n",
       " 'wane': 25245,\n",
       " 'edgeways': 52321,\n",
       " 'titanium': 34711,\n",
       " 'pinta': 40852,\n",
       " 'want': 178,\n",
       " 'pinto': 30572,\n",
       " 'whoopdedoodles': 52065,\n",
       " 'tchaikovsky': 21908,\n",
       " 'travel': 2103,\n",
       " \"'victory'\": 52066,\n",
       " 'copious': 11928,\n",
       " 'gouge': 22433,\n",
       " \"chapters'\": 52067,\n",
       " 'barbra': 6702,\n",
       " 'uselessness': 30573,\n",
       " \"wan'\": 52068,\n",
       " 'assimilated': 27635,\n",
       " 'petiot': 16116,\n",
       " 'most\\x85and': 52069,\n",
       " 'dinosaurs': 3930,\n",
       " 'wrong': 352,\n",
       " 'seda': 52070,\n",
       " 'stollen': 52071,\n",
       " 'sentencing': 34712,\n",
       " 'ouroboros': 40853,\n",
       " 'assimilates': 40854,\n",
       " 'colorfully': 40855,\n",
       " 'glenne': 27636,\n",
       " 'dongen': 52072,\n",
       " 'subplots': 4760,\n",
       " 'kiloton': 52073,\n",
       " 'chandon': 23381,\n",
       " \"effect'\": 34713,\n",
       " 'snugly': 27637,\n",
       " 'kuei': 40856,\n",
       " 'welcomed': 9092,\n",
       " 'dishonor': 30071,\n",
       " 'concurrence': 52075,\n",
       " 'stoicism': 23382,\n",
       " \"guys'\": 14896,\n",
       " \"beroemd'\": 52077,\n",
       " 'butcher': 6703,\n",
       " \"melfi's\": 40857,\n",
       " 'aargh': 30623,\n",
       " 'playhouse': 20599,\n",
       " 'wickedly': 11308,\n",
       " 'fit': 1180,\n",
       " 'labratory': 52078,\n",
       " 'lifeline': 40859,\n",
       " 'screaming': 1927,\n",
       " 'fix': 4287,\n",
       " 'cineliterate': 52079,\n",
       " 'fic': 52080,\n",
       " 'fia': 52081,\n",
       " 'fig': 34714,\n",
       " 'fmvs': 52082,\n",
       " 'fie': 52083,\n",
       " 'reentered': 52084,\n",
       " 'fin': 30574,\n",
       " 'doctresses': 52085,\n",
       " 'fil': 52086,\n",
       " 'zucker': 12606,\n",
       " 'ached': 31931,\n",
       " 'counsil': 52088,\n",
       " 'paterfamilias': 52089,\n",
       " 'songwriter': 13885,\n",
       " 'shivam': 34715,\n",
       " 'hurting': 9654,\n",
       " 'effects': 299,\n",
       " 'slauther': 52090,\n",
       " \"'flame'\": 52091,\n",
       " 'sommerset': 52092,\n",
       " 'interwhined': 52093,\n",
       " 'whacking': 27638,\n",
       " 'bartok': 52094,\n",
       " 'barton': 8775,\n",
       " 'frewer': 21909,\n",
       " \"fi'\": 52095,\n",
       " 'ingrid': 6192,\n",
       " 'stribor': 30575,\n",
       " 'approporiately': 52096,\n",
       " 'wobblyhand': 52097,\n",
       " 'tantalisingly': 52098,\n",
       " 'ankylosaurus': 52099,\n",
       " 'parasites': 17634,\n",
       " 'childen': 52100,\n",
       " \"jenkins'\": 52101,\n",
       " 'metafiction': 52102,\n",
       " 'golem': 17635,\n",
       " 'indiscretion': 40860,\n",
       " \"reeves'\": 23383,\n",
       " \"inamorata's\": 57781,\n",
       " 'brittannica': 52104,\n",
       " 'adapt': 7916,\n",
       " \"russo's\": 30576,\n",
       " 'guitarists': 48246,\n",
       " 'abbott': 10553,\n",
       " 'abbots': 40861,\n",
       " 'lanisha': 17649,\n",
       " 'magickal': 40863,\n",
       " 'mattter': 52105,\n",
       " \"'willy\": 52106,\n",
       " 'pumpkins': 34716,\n",
       " 'stuntpeople': 52107,\n",
       " 'estimate': 30577,\n",
       " 'ugghhh': 40864,\n",
       " 'gameplay': 11309,\n",
       " \"wern't\": 52108,\n",
       " \"n'sync\": 40865,\n",
       " 'sickeningly': 16117,\n",
       " 'chiara': 40866,\n",
       " 'disturbed': 4011,\n",
       " 'portmanteau': 40867,\n",
       " 'ineffectively': 52109,\n",
       " \"duchonvey's\": 82143,\n",
       " \"nasty'\": 37519,\n",
       " 'purpose': 1285,\n",
       " 'lazers': 52112,\n",
       " 'lightened': 28105,\n",
       " 'kaliganj': 52113,\n",
       " 'popularism': 52114,\n",
       " \"damme's\": 18511,\n",
       " 'stylistics': 30578,\n",
       " 'mindgaming': 52115,\n",
       " 'spoilerish': 46449,\n",
       " \"'corny'\": 52117,\n",
       " 'boerner': 34718,\n",
       " 'olds': 6792,\n",
       " 'bakelite': 52118,\n",
       " 'renovated': 27639,\n",
       " 'forrester': 27640,\n",
       " \"lumiere's\": 52119,\n",
       " 'gaskets': 52024,\n",
       " 'needed': 884,\n",
       " 'smight': 34719,\n",
       " 'master': 1297,\n",
       " \"edie's\": 25905,\n",
       " 'seeber': 40868,\n",
       " 'hiya': 52120,\n",
       " 'fuzziness': 52121,\n",
       " 'genesis': 14897,\n",
       " 'rewards': 12607,\n",
       " 'enthrall': 30579,\n",
       " \"'about\": 40869,\n",
       " \"recollection's\": 52122,\n",
       " 'mutilated': 11039,\n",
       " 'fatherlands': 52123,\n",
       " \"fischer's\": 52124,\n",
       " 'positively': 5399,\n",
       " '270': 34705,\n",
       " 'ahmed': 34720,\n",
       " 'zatoichi': 9836,\n",
       " 'bannister': 13886,\n",
       " 'anniversaries': 52127,\n",
       " \"helm's\": 30580,\n",
       " \"'work'\": 52128,\n",
       " 'exclaimed': 34721,\n",
       " \"'unfunny'\": 52129,\n",
       " '274': 52029,\n",
       " 'feeling': 544,\n",
       " \"wanda's\": 52131,\n",
       " 'dolan': 33266,\n",
       " '278': 52133,\n",
       " 'peacoat': 52134,\n",
       " 'brawny': 40870,\n",
       " 'mishra': 40871,\n",
       " 'worlders': 40872,\n",
       " 'protags': 52135,\n",
       " 'skullcap': 52136,\n",
       " 'dastagir': 57596,\n",
       " 'affairs': 5622,\n",
       " 'wholesome': 7799,\n",
       " 'hymen': 52137,\n",
       " 'paramedics': 25246,\n",
       " 'unpersons': 52138,\n",
       " 'heavyarms': 52139,\n",
       " 'affaire': 52140,\n",
       " 'coulisses': 52141,\n",
       " 'hymer': 40873,\n",
       " 'kremlin': 52142,\n",
       " 'shipments': 30581,\n",
       " 'pixilated': 52143,\n",
       " \"'00s\": 30582,\n",
       " 'diminishing': 18512,\n",
       " 'cinematic': 1357,\n",
       " 'resonates': 14898,\n",
       " 'simplify': 40874,\n",
       " \"nature'\": 40875,\n",
       " 'temptresses': 40876,\n",
       " 'reverence': 16822,\n",
       " 'resonated': 19502,\n",
       " 'dailey': 34722,\n",
       " '2\\x85': 52144,\n",
       " 'treize': 27641,\n",
       " 'majo': 52145,\n",
       " 'kiya': 21910,\n",
       " 'woolnough': 52146,\n",
       " 'thanatos': 39797,\n",
       " 'sandoval': 35731,\n",
       " 'dorama': 40879,\n",
       " \"o'shaughnessy\": 52147,\n",
       " 'tech': 4988,\n",
       " 'fugitives': 32018,\n",
       " 'teck': 30583,\n",
       " \"'e'\": 76125,\n",
       " 'doesn’t': 40881,\n",
       " 'purged': 52149,\n",
       " 'saying': 657,\n",
       " \"martians'\": 41095,\n",
       " 'norliss': 23418,\n",
       " 'dickey': 27642,\n",
       " 'dicker': 52152,\n",
       " \"'sependipity\": 52153,\n",
       " 'padded': 8422,\n",
       " 'ordell': 57792,\n",
       " \"sturges'\": 40882,\n",
       " 'independentcritics': 52154,\n",
       " 'tempted': 5745,\n",
       " \"atkinson's\": 34724,\n",
       " 'hounded': 25247,\n",
       " 'apace': 52155,\n",
       " 'clicked': 15494,\n",
       " \"'humor'\": 30584,\n",
       " \"martino's\": 17177,\n",
       " \"'supporting\": 52156,\n",
       " 'warmongering': 52032,\n",
       " \"zemeckis's\": 34725,\n",
       " 'lube': 21911,\n",
       " 'shocky': 52157,\n",
       " 'plate': 7476,\n",
       " 'plata': 40883,\n",
       " 'sturgess': 40884,\n",
       " \"nerds'\": 40885,\n",
       " 'plato': 20600,\n",
       " 'plath': 34726,\n",
       " 'platt': 40886,\n",
       " 'mcnab': 52159,\n",
       " 'clumsiness': 27643,\n",
       " 'altogether': 3899,\n",
       " 'massacring': 42584,\n",
       " 'bicenntinial': 52160,\n",
       " 'skaal': 40887,\n",
       " 'droning': 14360,\n",
       " 'lds': 8776,\n",
       " 'jaguar': 21912,\n",
       " \"cale's\": 34727,\n",
       " 'nicely': 1777,\n",
       " 'mummy': 4588,\n",
       " \"lot's\": 18513,\n",
       " 'patch': 10086,\n",
       " 'kerkhof': 50202,\n",
       " \"leader's\": 52161,\n",
       " \"'movie\": 27644,\n",
       " 'uncomfirmed': 52162,\n",
       " 'heirloom': 40888,\n",
       " 'wrangle': 47360,\n",
       " 'emotion\\x85': 52163,\n",
       " \"'stargate'\": 52164,\n",
       " 'pinoy': 40889,\n",
       " 'conchatta': 40890,\n",
       " 'broeke': 41128,\n",
       " 'advisedly': 40891,\n",
       " \"barker's\": 17636,\n",
       " 'descours': 52166,\n",
       " 'lots': 772,\n",
       " 'lotr': 9259,\n",
       " 'irs': 9879,\n",
       " 'lott': 52167,\n",
       " 'xvi': 40892,\n",
       " 'irk': 34728,\n",
       " 'irl': 52168,\n",
       " 'ira': 6887,\n",
       " 'belzer': 21913,\n",
       " 'irc': 52169,\n",
       " 'ire': 27645,\n",
       " 'requisites': 40893,\n",
       " 'discipline': 7693,\n",
       " 'lyoko': 52961,\n",
       " 'extend': 11310,\n",
       " 'nature': 873,\n",
       " \"'dickie'\": 52170,\n",
       " 'optimist': 40894,\n",
       " 'lapping': 30586,\n",
       " 'superficial': 3900,\n",
       " 'vestment': 52171,\n",
       " 'extent': 2823,\n",
       " 'tendons': 52172,\n",
       " \"heller's\": 52173,\n",
       " 'quagmires': 52174,\n",
       " 'miyako': 52175,\n",
       " 'moocow': 20601,\n",
       " \"coles'\": 52176,\n",
       " 'lookit': 40895,\n",
       " 'ravenously': 52177,\n",
       " 'levitating': 40896,\n",
       " 'perfunctorily': 52178,\n",
       " 'lookin': 30587,\n",
       " \"lot'\": 40898,\n",
       " 'lookie': 52179,\n",
       " 'fearlessly': 34870,\n",
       " 'libyan': 52181,\n",
       " 'fondles': 40899,\n",
       " 'gopher': 35714,\n",
       " 'wearying': 40901,\n",
       " \"nz's\": 52182,\n",
       " 'minuses': 27646,\n",
       " 'puposelessly': 52183,\n",
       " 'shandling': 52184,\n",
       " 'decapitates': 31268,\n",
       " 'humming': 11929,\n",
       " \"'nother\": 40902,\n",
       " 'smackdown': 21914,\n",
       " 'underdone': 30588,\n",
       " 'frf': 40903,\n",
       " 'triviality': 52185,\n",
       " 'fro': 25248,\n",
       " 'bothers': 8777,\n",
       " \"'kensington\": 52186,\n",
       " 'much': 73,\n",
       " 'muco': 34730,\n",
       " 'wiseguy': 22615,\n",
       " \"richie's\": 27648,\n",
       " 'tonino': 40904,\n",
       " 'unleavened': 52187,\n",
       " 'fry': 11587,\n",
       " \"'tv'\": 40905,\n",
       " 'toning': 40906,\n",
       " 'obese': 14361,\n",
       " 'sensationalized': 30589,\n",
       " 'spiv': 40907,\n",
       " 'spit': 6259,\n",
       " 'arkin': 7364,\n",
       " 'charleton': 21915,\n",
       " 'jeon': 16823,\n",
       " 'boardroom': 21916,\n",
       " 'doubts': 4989,\n",
       " 'spin': 3084,\n",
       " 'hepo': 53083,\n",
       " 'wildcat': 27649,\n",
       " 'venoms': 10584,\n",
       " 'misconstrues': 52191,\n",
       " 'mesmerising': 18514,\n",
       " 'misconstrued': 40908,\n",
       " 'rescinds': 52192,\n",
       " 'prostrate': 52193,\n",
       " 'majid': 40909,\n",
       " 'climbed': 16479,\n",
       " 'canoeing': 34731,\n",
       " 'majin': 52195,\n",
       " 'animie': 57804,\n",
       " 'sylke': 40910,\n",
       " 'conditioned': 14899,\n",
       " 'waddell': 40911,\n",
       " '3\\x85': 52196,\n",
       " 'hyperdrive': 41188,\n",
       " 'conditioner': 34732,\n",
       " 'bricklayer': 53153,\n",
       " 'hong': 2576,\n",
       " 'memoriam': 52198,\n",
       " 'inventively': 30592,\n",
       " \"levant's\": 25249,\n",
       " 'portobello': 20638,\n",
       " 'remand': 52200,\n",
       " 'mummified': 19504,\n",
       " 'honk': 27650,\n",
       " 'spews': 19505,\n",
       " 'visitations': 40912,\n",
       " 'mummifies': 52201,\n",
       " 'cavanaugh': 25250,\n",
       " 'zeon': 23385,\n",
       " \"jungle's\": 40913,\n",
       " 'viertel': 34733,\n",
       " 'frenchmen': 27651,\n",
       " 'torpedoes': 52202,\n",
       " 'schlessinger': 52203,\n",
       " 'torpedoed': 34734,\n",
       " 'blister': 69876,\n",
       " 'cinefest': 52204,\n",
       " 'furlough': 34735,\n",
       " 'mainsequence': 52205,\n",
       " 'mentors': 40914,\n",
       " 'academic': 9094,\n",
       " 'stillness': 20602,\n",
       " 'academia': 40915,\n",
       " 'lonelier': 52206,\n",
       " 'nibby': 52207,\n",
       " \"losers'\": 52208,\n",
       " 'cineastes': 40916,\n",
       " 'corporate': 4449,\n",
       " 'massaging': 40917,\n",
       " 'bellow': 30593,\n",
       " 'absurdities': 19506,\n",
       " 'expetations': 53241,\n",
       " 'nyfiken': 40918,\n",
       " 'mehras': 75638,\n",
       " 'lasse': 52209,\n",
       " 'visability': 52210,\n",
       " 'militarily': 33946,\n",
       " \"elder'\": 52211,\n",
       " 'gainsbourg': 19023,\n",
       " 'hah': 20603,\n",
       " 'hai': 13420,\n",
       " 'haj': 34736,\n",
       " 'hak': 25251,\n",
       " 'hal': 4311,\n",
       " 'ham': 4892,\n",
       " 'duffer': 53259,\n",
       " 'haa': 52213,\n",
       " 'had': 66,\n",
       " 'advancement': 11930,\n",
       " 'hag': 16825,\n",
       " \"hand'\": 25252,\n",
       " 'hay': 13421,\n",
       " 'mcnamara': 20604,\n",
       " \"mozart's\": 52214,\n",
       " 'duffel': 30731,\n",
       " 'haq': 30594,\n",
       " 'har': 13887,\n",
       " 'has': 44,\n",
       " 'hat': 2401,\n",
       " 'hav': 40919,\n",
       " 'haw': 30595,\n",
       " 'figtings': 52215,\n",
       " 'elders': 15495,\n",
       " 'underpanted': 52216,\n",
       " 'pninson': 52217,\n",
       " 'unequivocally': 27652,\n",
       " \"barbara's\": 23673,\n",
       " \"bello'\": 52219,\n",
       " 'indicative': 12997,\n",
       " 'yawnfest': 40920,\n",
       " 'hexploitation': 52220,\n",
       " \"loder's\": 52221,\n",
       " 'sleuthing': 27653,\n",
       " \"justin's\": 32622,\n",
       " \"'ball\": 52222,\n",
       " \"'summer\": 52223,\n",
       " \"'demons'\": 34935,\n",
       " \"mormon's\": 52225,\n",
       " \"laughton's\": 34737,\n",
       " 'debell': 52226,\n",
       " 'shipyard': 39724,\n",
       " 'unabashedly': 30597,\n",
       " 'disks': 40401,\n",
       " 'crowd': 2290,\n",
       " 'crowe': 10087,\n",
       " \"vancouver's\": 56434,\n",
       " 'mosques': 34738,\n",
       " 'crown': 6627,\n",
       " 'culpas': 52227,\n",
       " 'crows': 27654,\n",
       " 'surrell': 53344,\n",
       " 'flowless': 52229,\n",
       " 'sheirk': 52230,\n",
       " \"'three\": 40923,\n",
       " \"peterson'\": 52231,\n",
       " 'ooverall': 52232,\n",
       " 'perchance': 40924,\n",
       " 'bottom': 1321,\n",
       " 'chabert': 53363,\n",
       " 'sneha': 52233,\n",
       " 'inhuman': 13888,\n",
       " 'ichii': 52234,\n",
       " 'ursla': 52235,\n",
       " 'completly': 30598,\n",
       " 'moviedom': 40925,\n",
       " 'raddick': 52236,\n",
       " 'brundage': 51995,\n",
       " 'brigades': 40926,\n",
       " 'starring': 1181,\n",
       " \"'goal'\": 52237,\n",
       " 'caskets': 52238,\n",
       " 'willcock': 52239,\n",
       " \"threesome's\": 52240,\n",
       " \"mosque'\": 52241,\n",
       " \"cover's\": 52242,\n",
       " 'spaceships': 17637,\n",
       " 'anomalous': 40927,\n",
       " 'ptsd': 27655,\n",
       " 'shirdan': 52243,\n",
       " 'obscenity': 21962,\n",
       " 'lemmings': 30599,\n",
       " 'duccio': 30600,\n",
       " \"levene's\": 52244,\n",
       " \"'gorby'\": 52245,\n",
       " \"teenager's\": 25255,\n",
       " 'marshall': 5340,\n",
       " 'honeymoon': 9095,\n",
       " 'shoots': 3231,\n",
       " 'despised': 12258,\n",
       " 'okabasho': 52246,\n",
       " 'fabric': 8289,\n",
       " 'cannavale': 18515,\n",
       " 'raped': 3537,\n",
       " \"tutt's\": 52247,\n",
       " 'grasping': 17638,\n",
       " 'despises': 18516,\n",
       " \"thief's\": 40928,\n",
       " 'rapes': 8926,\n",
       " 'raper': 52248,\n",
       " \"eyre'\": 27656,\n",
       " 'walchek': 52249,\n",
       " \"elmo's\": 23386,\n",
       " 'perfumes': 40929,\n",
       " 'spurting': 21918,\n",
       " \"exposition'\\x85\": 52250,\n",
       " 'denoting': 52251,\n",
       " 'thesaurus': 34740,\n",
       " \"shoot'\": 40930,\n",
       " 'bonejack': 49759,\n",
       " 'simpsonian': 52253,\n",
       " 'hebetude': 30601,\n",
       " \"hallow's\": 34741,\n",
       " 'desperation\\x85': 52254,\n",
       " 'incinerator': 34742,\n",
       " 'congratulations': 10308,\n",
       " 'humbled': 52255,\n",
       " \"else's\": 5924,\n",
       " 'trelkovski': 40845,\n",
       " \"rape'\": 52256,\n",
       " \"'chapters'\": 59386,\n",
       " '1600s': 52257,\n",
       " 'martian': 7253,\n",
       " 'nicest': 25256,\n",
       " 'eyred': 52259,\n",
       " 'passenger': 9457,\n",
       " 'disgrace': 6041,\n",
       " 'moderne': 52260,\n",
       " 'barrymore': 5120,\n",
       " 'yankovich': 52261,\n",
       " 'moderns': 40931,\n",
       " 'studliest': 52262,\n",
       " 'bedsheet': 52263,\n",
       " 'decapitation': 14900,\n",
       " 'slurring': 52264,\n",
       " \"'nunsploitation'\": 52265,\n",
       " \"'character'\": 34743,\n",
       " 'cambodia': 9880,\n",
       " 'rebelious': 52266,\n",
       " 'pasadena': 27657,\n",
       " 'crowne': 40932,\n",
       " \"'bedchamber\": 52267,\n",
       " 'conjectural': 52268,\n",
       " 'appologize': 52269,\n",
       " 'halfassing': 52270,\n",
       " 'paycheque': 57816,\n",
       " 'palms': 20606,\n",
       " \"'islands\": 52271,\n",
       " 'hawked': 40933,\n",
       " 'palme': 21919,\n",
       " 'conservatively': 40934,\n",
       " 'larp': 64007,\n",
       " 'palma': 5558,\n",
       " 'smelling': 21920,\n",
       " 'aragorn': 12998,\n",
       " 'hawker': 52272,\n",
       " 'hawkes': 52273,\n",
       " 'explosions': 3975,\n",
       " 'loren': 8059,\n",
       " \"pyle's\": 52274,\n",
       " 'shootout': 6704,\n",
       " \"mike's\": 18517,\n",
       " \"driscoll's\": 52275,\n",
       " 'cogsworth': 40935,\n",
       " \"britian's\": 52276,\n",
       " 'childs': 34744,\n",
       " \"portrait's\": 52277,\n",
       " 'chain': 3626,\n",
       " 'whoever': 2497,\n",
       " 'puttered': 52278,\n",
       " 'childe': 52279,\n",
       " 'maywether': 52280,\n",
       " 'chair': 3036,\n",
       " \"rance's\": 52281,\n",
       " 'machu': 34745,\n",
       " 'ballet': 4517,\n",
       " 'grapples': 34746,\n",
       " 'summerize': 76152,\n",
       " 'freelance': 30603,\n",
       " \"andrea's\": 52283,\n",
       " '\\x91very': 52284,\n",
       " 'coolidge': 45879,\n",
       " 'mache': 18518,\n",
       " 'balled': 52285,\n",
       " 'grappled': 40937,\n",
       " 'macha': 18519,\n",
       " 'underlining': 21921,\n",
       " 'macho': 5623,\n",
       " 'oversight': 19507,\n",
       " 'machi': 25257,\n",
       " 'verbally': 11311,\n",
       " 'tenacious': 21922,\n",
       " 'windshields': 40938,\n",
       " 'paychecks': 18557,\n",
       " 'jerk': 3396,\n",
       " \"good'\": 11931,\n",
       " 'prancer': 34748,\n",
       " 'prances': 21923,\n",
       " 'olympus': 52286,\n",
       " 'lark': 21924,\n",
       " 'embark': 10785,\n",
       " 'gloomy': 7365,\n",
       " 'jehaan': 52287,\n",
       " 'turaqui': 52288,\n",
       " \"child'\": 20607,\n",
       " 'locked': 2894,\n",
       " 'pranced': 52289,\n",
       " 'exact': 2588,\n",
       " 'unattuned': 52290,\n",
       " 'minute': 783,\n",
       " 'skewed': 16118,\n",
       " 'hodgins': 40940,\n",
       " 'skewer': 34749,\n",
       " 'think\\x85': 52291,\n",
       " 'rosenstein': 38765,\n",
       " 'helmit': 52292,\n",
       " 'wrestlemanias': 34750,\n",
       " 'hindered': 16826,\n",
       " \"martha's\": 30604,\n",
       " 'cheree': 52293,\n",
       " \"pluckin'\": 52294,\n",
       " 'ogles': 40941,\n",
       " 'heavyweight': 11932,\n",
       " 'aada': 82190,\n",
       " 'chopping': 11312,\n",
       " 'strongboy': 61534,\n",
       " 'hegemonic': 41342,\n",
       " 'adorns': 40942,\n",
       " 'xxth': 41346,\n",
       " 'nobuhiro': 34751,\n",
       " 'capitães': 52298,\n",
       " 'kavogianni': 52299,\n",
       " 'antwerp': 13422,\n",
       " 'celebrated': 6538,\n",
       " 'roarke': 52300,\n",
       " 'baggins': 40943,\n",
       " 'cheeseburgers': 31270,\n",
       " 'matras': 52301,\n",
       " \"nineties'\": 52302,\n",
       " \"'craig'\": 52303,\n",
       " 'celebrates': 12999,\n",
       " 'unintentionally': 3383,\n",
       " 'drafted': 14362,\n",
       " 'climby': 52304,\n",
       " '303': 52305,\n",
       " 'oldies': 18520,\n",
       " 'climbs': 9096,\n",
       " 'honour': 9655,\n",
       " 'plucking': 34752,\n",
       " '305': 30074,\n",
       " 'address': 5514,\n",
       " 'menjou': 40944,\n",
       " \"'freak'\": 42592,\n",
       " 'dwindling': 19508,\n",
       " 'benson': 9458,\n",
       " 'white’s': 52307,\n",
       " 'shamelessness': 40945,\n",
       " 'impacted': 21925,\n",
       " 'upatz': 52308,\n",
       " 'cusack': 3840,\n",
       " \"flavia's\": 37567,\n",
       " 'effette': 52309,\n",
       " 'influx': 34753,\n",
       " 'boooooooo': 52310,\n",
       " 'dimitrova': 52311,\n",
       " 'houseman': 13423,\n",
       " 'bigas': 25259,\n",
       " 'boylen': 52312,\n",
       " 'phillipenes': 52313,\n",
       " 'fakery': 40946,\n",
       " \"grandpa's\": 27658,\n",
       " 'darnell': 27659,\n",
       " 'undergone': 19509,\n",
       " 'handbags': 52315,\n",
       " 'perished': 21926,\n",
       " 'pooped': 37778,\n",
       " 'vigour': 27660,\n",
       " 'opposed': 3627,\n",
       " 'etude': 52316,\n",
       " \"caine's\": 11799,\n",
       " 'doozers': 52317,\n",
       " 'photojournals': 34754,\n",
       " 'perishes': 52318,\n",
       " 'constrains': 34755,\n",
       " 'migenes': 40948,\n",
       " 'consoled': 30605,\n",
       " 'alastair': 16827,\n",
       " 'wvs': 52319,\n",
       " 'ooooooh': 52320,\n",
       " 'approving': 34756,\n",
       " 'consoles': 40949,\n",
       " 'disparagement': 52064,\n",
       " 'futureistic': 52322,\n",
       " 'rebounding': 52323,\n",
       " \"'date\": 52324,\n",
       " 'gregoire': 52325,\n",
       " 'rutherford': 21927,\n",
       " 'americanised': 34757,\n",
       " 'novikov': 82196,\n",
       " 'following': 1042,\n",
       " 'munroe': 34758,\n",
       " \"morita'\": 52326,\n",
       " 'christenssen': 52327,\n",
       " 'oatmeal': 23106,\n",
       " 'fossey': 25260,\n",
       " 'livered': 40950,\n",
       " 'listens': 13000,\n",
       " \"'marci\": 76164,\n",
       " \"otis's\": 52330,\n",
       " 'thanking': 23387,\n",
       " 'maude': 16019,\n",
       " 'extensions': 34759,\n",
       " 'ameteurish': 52332,\n",
       " \"commender's\": 52333,\n",
       " 'agricultural': 27661,\n",
       " 'convincingly': 4518,\n",
       " 'fueled': 17639,\n",
       " 'mahattan': 54014,\n",
       " \"paris's\": 40952,\n",
       " 'vulkan': 52336,\n",
       " 'stapes': 52337,\n",
       " 'odysessy': 52338,\n",
       " 'harmon': 12259,\n",
       " 'surfing': 4252,\n",
       " 'halloran': 23494,\n",
       " 'unbelieveably': 49580,\n",
       " \"'offed'\": 52339,\n",
       " 'quadrant': 30607,\n",
       " 'inhabiting': 19510,\n",
       " 'nebbish': 34760,\n",
       " 'forebears': 40953,\n",
       " 'skirmish': 34761,\n",
       " 'ocassionally': 52340,\n",
       " \"'resist\": 52341,\n",
       " 'impactful': 21928,\n",
       " 'spicier': 52342,\n",
       " 'touristy': 40954,\n",
       " \"'football'\": 52343,\n",
       " 'webpage': 40955,\n",
       " 'exurbia': 52345,\n",
       " 'jucier': 52346,\n",
       " 'professors': 14901,\n",
       " 'structuring': 34762,\n",
       " 'jig': 30608,\n",
       " 'overlord': 40956,\n",
       " 'disconnect': 25261,\n",
       " 'sniffle': 82201,\n",
       " 'slimeball': 40957,\n",
       " 'jia': 40958,\n",
       " 'milked': 16828,\n",
       " 'banjoes': 40959,\n",
       " 'jim': 1237,\n",
       " 'workforces': 52348,\n",
       " 'jip': 52349,\n",
       " 'rotweiller': 52350,\n",
       " 'mundaneness': 34763,\n",
       " \"'ninja'\": 52351,\n",
       " \"dead'\": 11040,\n",
       " \"cipriani's\": 40960,\n",
       " 'modestly': 20608,\n",
       " \"professor'\": 52352,\n",
       " 'shacked': 40961,\n",
       " 'bashful': 34764,\n",
       " 'sorter': 23388,\n",
       " 'overpowering': 16120,\n",
       " 'workmanlike': 18521,\n",
       " 'henpecked': 27662,\n",
       " 'sorted': 18522,\n",
       " \"jōb's\": 52354,\n",
       " \"'always\": 52355,\n",
       " \"'baptists\": 34765,\n",
       " 'dreamcatchers': 52356,\n",
       " \"'silence'\": 52357,\n",
       " 'hickory': 21929,\n",
       " 'fun\\x97yet': 52358,\n",
       " 'breakumentary': 52359,\n",
       " 'didn': 15496,\n",
       " 'didi': 52360,\n",
       " 'pealing': 52361,\n",
       " 'dispite': 40962,\n",
       " \"italy's\": 25262,\n",
       " 'instability': 21930,\n",
       " 'quarter': 6539,\n",
       " 'quartet': 12608,\n",
       " 'padmé': 52362,\n",
       " \"'bleedmedry\": 52363,\n",
       " 'pahalniuk': 52364,\n",
       " 'honduras': 52365,\n",
       " 'bursting': 10786,\n",
       " \"pablo's\": 41465,\n",
       " 'irremediably': 52367,\n",
       " 'presages': 40963,\n",
       " 'bowlegged': 57832,\n",
       " 'dalip': 65183,\n",
       " 'entering': 6260,\n",
       " 'newsradio': 76172,\n",
       " 'presaged': 54150,\n",
       " \"giallo's\": 27663,\n",
       " 'bouyant': 40964,\n",
       " 'amerterish': 52368,\n",
       " 'rajni': 18523,\n",
       " 'leeves': 30610,\n",
       " 'macauley': 34767,\n",
       " 'seriously': 612,\n",
       " 'sugercoma': 52369,\n",
       " 'grimstead': 52370,\n",
       " \"'fairy'\": 52371,\n",
       " 'zenda': 30611,\n",
       " \"'twins'\": 52372,\n",
       " 'realisation': 17640,\n",
       " 'highsmith': 27664,\n",
       " 'raunchy': 7817,\n",
       " 'incentives': 40965,\n",
       " 'flatson': 52374,\n",
       " 'snooker': 35097,\n",
       " 'crazies': 16829,\n",
       " 'crazier': 14902,\n",
       " 'grandma': 7094,\n",
       " 'napunsaktha': 52375,\n",
       " 'workmanship': 30612,\n",
       " 'reisner': 52376,\n",
       " \"sanford's\": 61306,\n",
       " '\\x91doña': 52377,\n",
       " 'modest': 6108,\n",
       " \"everything's\": 19153,\n",
       " 'hamer': 40966,\n",
       " \"couldn't'\": 52379,\n",
       " 'quibble': 13001,\n",
       " 'socking': 52380,\n",
       " 'tingler': 21931,\n",
       " 'gutman': 52381,\n",
       " 'lachlan': 40967,\n",
       " 'tableaus': 52382,\n",
       " 'headbanger': 52383,\n",
       " 'spoken': 2847,\n",
       " 'cerebrally': 34768,\n",
       " \"'road\": 23490,\n",
       " 'tableaux': 21932,\n",
       " \"proust's\": 40968,\n",
       " 'periodical': 40969,\n",
       " \"shoveller's\": 52385,\n",
       " 'tamara': 25263,\n",
       " 'affords': 17641,\n",
       " 'concert': 3249,\n",
       " \"yara's\": 87955,\n",
       " 'someome': 52386,\n",
       " 'lingering': 8424,\n",
       " \"abraham's\": 41511,\n",
       " 'beesley': 34769,\n",
       " 'cherbourg': 34770,\n",
       " 'kagan': 28624,\n",
       " 'snatch': 9097,\n",
       " \"miyazaki's\": 9260,\n",
       " 'absorbs': 25264,\n",
       " \"koltai's\": 40970,\n",
       " 'tingled': 64027,\n",
       " 'crossroads': 19511,\n",
       " 'rehab': 16121,\n",
       " 'falworth': 52389,\n",
       " 'sequals': 52390,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어와 정수 인덱스를 매핑한 딕셔너리\n",
    "word_index = imdb.get_word_index()\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비\n",
    "리뷰-정수 배열은 신경망에 주입하기 전에 텐서로 변환해야 함. 변환 방법은 몇가지가 있음\n",
    "* 원-핫 인코딩(one-hot encoding) : 정수 배열을 0과 1로 이루어진 벡터로 변환하는 방법. 배열 [3, 5]을 인덱스 3과 5만 1이고 나머지가 모두 0인 10000차원 벡터로 변환할 수 있음. 이 방법은 `num_words * num_reviews`크기의 행렬이 필요하기 때문에 메모리가 많이 필요함\n",
    "* 다른 방법으로는, 정수 배열의 길이가 모두 같도록 패딩(padding)을 추가해 `max_length * num_reviews`크기의 정수 텐서를 만들어 사용. 이런 형태의 텐서를 다룰 수 있는 임베딩(embedding)층을 신경망의 첫 번째 층으로 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)\n",
    "* sequences : list나 lists, 해당 값들은 sequence\n",
    "* maxlen : (Int)모든 sequences들의 max lenth\n",
    "* padding : 'pre' 혹은 'post', sequence 앞 혹은 뒤에 pad값 입력\n",
    "* value : padding 자리에 넣을 값, Float or String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뒤에 0이 채워진 것을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 입력 크기는 영화 리뷰 데이터셋에 적용된 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 첫 번째 층은 `Embedding`층으로 정수로 인코딩된 단어를 입력받아 각 인덱스에 해당하는 임베딩 벡터를 학습한다. 입력은 `(batch, sequence_length)`이며 최종 출력은 `(batch_size, sequence_length, output_dim)`형태이다. \n",
    "2. `GlobalAveragePooling1D`층은 `sequence`차원에 대한 평균을 계산하여 각 샘플에 대해 고정된 길이의 출력 벡터를 반환. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수 및 옵티마이저 설정\n",
    "여기서는 긍정/부정 두 가지에 대한 분류 문제이고 확률을 출력하므로 `binary_crossentropy`손실 함수를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검증 데이터셋 만들기\n",
    "원본 데이터(train_data)에서 10000개의 샘플을 떼어내어 검증(validation set)을 만들어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - ETA: 25s - loss: 0.6930 - accuracy: 0.494 - ETA: 6s - loss: 0.6930 - accuracy: 0.491 - ETA: 3s - loss: 0.6930 - accuracy: 0.49 - ETA: 2s - loss: 0.6930 - accuracy: 0.50 - ETA: 1s - loss: 0.6929 - accuracy: 0.50 - ETA: 1s - loss: 0.6928 - accuracy: 0.50 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.53 - ETA: 0s - loss: 0.6923 - accuracy: 0.54 - 2s 109us/sample - loss: 0.6922 - accuracy: 0.5499 - val_loss: 0.6903 - val_accuracy: 0.6519\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.6900 - accuracy: 0.66 - ETA: 0s - loss: 0.6896 - accuracy: 0.67 - ETA: 0s - loss: 0.6893 - accuracy: 0.69 - ETA: 0s - loss: 0.6893 - accuracy: 0.68 - ETA: 0s - loss: 0.6889 - accuracy: 0.69 - ETA: 0s - loss: 0.6887 - accuracy: 0.69 - ETA: 0s - loss: 0.6883 - accuracy: 0.69 - ETA: 0s - loss: 0.6880 - accuracy: 0.70 - ETA: 0s - loss: 0.6878 - accuracy: 0.70 - ETA: 0s - loss: 0.6874 - accuracy: 0.70 - 1s 46us/sample - loss: 0.6871 - accuracy: 0.7111 - val_loss: 0.6833 - val_accuracy: 0.7436\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.6827 - accuracy: 0.74 - ETA: 0s - loss: 0.6821 - accuracy: 0.74 - ETA: 0s - loss: 0.6812 - accuracy: 0.74 - ETA: 0s - loss: 0.6806 - accuracy: 0.74 - ETA: 0s - loss: 0.6802 - accuracy: 0.73 - ETA: 0s - loss: 0.6797 - accuracy: 0.72 - ETA: 0s - loss: 0.6791 - accuracy: 0.72 - ETA: 0s - loss: 0.6784 - accuracy: 0.73 - ETA: 0s - loss: 0.6777 - accuracy: 0.73 - ETA: 0s - loss: 0.6768 - accuracy: 0.73 - 1s 47us/sample - loss: 0.6765 - accuracy: 0.7393 - val_loss: 0.6704 - val_accuracy: 0.7496\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.6673 - accuracy: 0.77 - ETA: 0s - loss: 0.6669 - accuracy: 0.76 - ETA: 0s - loss: 0.6662 - accuracy: 0.76 - ETA: 0s - loss: 0.6648 - accuracy: 0.76 - ETA: 0s - loss: 0.6640 - accuracy: 0.75 - ETA: 0s - loss: 0.6629 - accuracy: 0.75 - ETA: 0s - loss: 0.6616 - accuracy: 0.76 - ETA: 0s - loss: 0.6606 - accuracy: 0.76 - ETA: 0s - loss: 0.6596 - accuracy: 0.76 - ETA: 0s - loss: 0.6584 - accuracy: 0.76 - 1s 46us/sample - loss: 0.6576 - accuracy: 0.7673 - val_loss: 0.6485 - val_accuracy: 0.7636\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.6419 - accuracy: 0.80 - ETA: 0s - loss: 0.6436 - accuracy: 0.77 - ETA: 0s - loss: 0.6404 - accuracy: 0.78 - ETA: 0s - loss: 0.6376 - accuracy: 0.78 - ETA: 0s - loss: 0.6367 - accuracy: 0.78 - ETA: 0s - loss: 0.6353 - accuracy: 0.78 - ETA: 0s - loss: 0.6336 - accuracy: 0.79 - ETA: 0s - loss: 0.6318 - accuracy: 0.79 - ETA: 0s - loss: 0.6300 - accuracy: 0.79 - ETA: 0s - loss: 0.6287 - accuracy: 0.79 - 1s 46us/sample - loss: 0.6283 - accuracy: 0.7906 - val_loss: 0.6160 - val_accuracy: 0.7845\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.6106 - accuracy: 0.78 - ETA: 0s - loss: 0.6076 - accuracy: 0.79 - ETA: 0s - loss: 0.6055 - accuracy: 0.79 - ETA: 0s - loss: 0.6025 - accuracy: 0.80 - ETA: 0s - loss: 0.5998 - accuracy: 0.80 - ETA: 0s - loss: 0.5980 - accuracy: 0.80 - ETA: 0s - loss: 0.5964 - accuracy: 0.80 - ETA: 0s - loss: 0.5933 - accuracy: 0.80 - ETA: 0s - loss: 0.5915 - accuracy: 0.80 - ETA: 0s - loss: 0.5895 - accuracy: 0.80 - 1s 45us/sample - loss: 0.5888 - accuracy: 0.8056 - val_loss: 0.5771 - val_accuracy: 0.7975\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.5652 - accuracy: 0.81 - ETA: 0s - loss: 0.5596 - accuracy: 0.82 - ETA: 0s - loss: 0.5580 - accuracy: 0.81 - ETA: 0s - loss: 0.5554 - accuracy: 0.82 - ETA: 0s - loss: 0.5533 - accuracy: 0.82 - ETA: 0s - loss: 0.5514 - accuracy: 0.82 - ETA: 0s - loss: 0.5491 - accuracy: 0.82 - ETA: 0s - loss: 0.5489 - accuracy: 0.82 - ETA: 0s - loss: 0.5471 - accuracy: 0.81 - ETA: 0s - loss: 0.5441 - accuracy: 0.82 - 1s 46us/sample - loss: 0.5429 - accuracy: 0.8212 - val_loss: 0.5344 - val_accuracy: 0.8141\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.5219 - accuracy: 0.85 - ETA: 0s - loss: 0.5189 - accuracy: 0.83 - ETA: 0s - loss: 0.5137 - accuracy: 0.84 - ETA: 0s - loss: 0.5094 - accuracy: 0.84 - ETA: 0s - loss: 0.5080 - accuracy: 0.84 - ETA: 0s - loss: 0.5067 - accuracy: 0.83 - ETA: 0s - loss: 0.5048 - accuracy: 0.83 - ETA: 0s - loss: 0.5021 - accuracy: 0.83 - ETA: 0s - loss: 0.4989 - accuracy: 0.83 - ETA: 0s - loss: 0.4963 - accuracy: 0.84 - 1s 45us/sample - loss: 0.4954 - accuracy: 0.8411 - val_loss: 0.4919 - val_accuracy: 0.8272\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.4425 - accuracy: 0.89 - ETA: 0s - loss: 0.4618 - accuracy: 0.85 - ETA: 0s - loss: 0.4640 - accuracy: 0.84 - ETA: 0s - loss: 0.4588 - accuracy: 0.85 - ETA: 0s - loss: 0.4586 - accuracy: 0.85 - ETA: 0s - loss: 0.4568 - accuracy: 0.85 - ETA: 0s - loss: 0.4552 - accuracy: 0.85 - ETA: 0s - loss: 0.4544 - accuracy: 0.85 - ETA: 0s - loss: 0.4537 - accuracy: 0.85 - ETA: 0s - loss: 0.4510 - accuracy: 0.85 - 1s 45us/sample - loss: 0.4504 - accuracy: 0.8551 - val_loss: 0.4539 - val_accuracy: 0.8378\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.4159 - accuracy: 0.85 - ETA: 0s - loss: 0.4171 - accuracy: 0.86 - ETA: 0s - loss: 0.4204 - accuracy: 0.86 - ETA: 0s - loss: 0.4225 - accuracy: 0.86 - ETA: 0s - loss: 0.4223 - accuracy: 0.86 - ETA: 0s - loss: 0.4194 - accuracy: 0.86 - ETA: 0s - loss: 0.4174 - accuracy: 0.86 - ETA: 0s - loss: 0.4155 - accuracy: 0.86 - ETA: 0s - loss: 0.4135 - accuracy: 0.86 - ETA: 0s - loss: 0.4105 - accuracy: 0.86 - 1s 45us/sample - loss: 0.4101 - accuracy: 0.8686 - val_loss: 0.4207 - val_accuracy: 0.8492\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.86 - ETA: 0s - loss: 0.3774 - accuracy: 0.87 - ETA: 0s - loss: 0.3799 - accuracy: 0.88 - ETA: 0s - loss: 0.3832 - accuracy: 0.87 - ETA: 0s - loss: 0.3823 - accuracy: 0.87 - ETA: 0s - loss: 0.3824 - accuracy: 0.87 - ETA: 0s - loss: 0.3801 - accuracy: 0.87 - ETA: 0s - loss: 0.3794 - accuracy: 0.87 - ETA: 0s - loss: 0.3781 - accuracy: 0.87 - ETA: 0s - loss: 0.3763 - accuracy: 0.87 - 1s 46us/sample - loss: 0.3755 - accuracy: 0.8776 - val_loss: 0.3944 - val_accuracy: 0.8549\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.3380 - accuracy: 0.90 - ETA: 0s - loss: 0.3539 - accuracy: 0.88 - ETA: 0s - loss: 0.3553 - accuracy: 0.88 - ETA: 0s - loss: 0.3538 - accuracy: 0.88 - ETA: 0s - loss: 0.3512 - accuracy: 0.88 - ETA: 0s - loss: 0.3497 - accuracy: 0.88 - ETA: 0s - loss: 0.3504 - accuracy: 0.88 - ETA: 0s - loss: 0.3500 - accuracy: 0.88 - ETA: 0s - loss: 0.3484 - accuracy: 0.88 - ETA: 0s - loss: 0.3474 - accuracy: 0.88 - 1s 45us/sample - loss: 0.3466 - accuracy: 0.8857 - val_loss: 0.3728 - val_accuracy: 0.8602\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.88 - ETA: 0s - loss: 0.3365 - accuracy: 0.89 - ETA: 0s - loss: 0.3321 - accuracy: 0.89 - ETA: 0s - loss: 0.3266 - accuracy: 0.89 - ETA: 0s - loss: 0.3299 - accuracy: 0.88 - ETA: 0s - loss: 0.3273 - accuracy: 0.89 - ETA: 0s - loss: 0.3258 - accuracy: 0.89 - ETA: 0s - loss: 0.3241 - accuracy: 0.89 - ETA: 0s - loss: 0.3245 - accuracy: 0.89 - ETA: 0s - loss: 0.3230 - accuracy: 0.89 - 1s 45us/sample - loss: 0.3227 - accuracy: 0.8925 - val_loss: 0.3560 - val_accuracy: 0.8667\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.3157 - accuracy: 0.90 - ETA: 0s - loss: 0.3109 - accuracy: 0.89 - ETA: 0s - loss: 0.3090 - accuracy: 0.89 - ETA: 0s - loss: 0.3045 - accuracy: 0.90 - ETA: 0s - loss: 0.3036 - accuracy: 0.90 - ETA: 0s - loss: 0.3034 - accuracy: 0.89 - ETA: 0s - loss: 0.3052 - accuracy: 0.89 - ETA: 0s - loss: 0.3044 - accuracy: 0.89 - ETA: 0s - loss: 0.3030 - accuracy: 0.89 - ETA: 0s - loss: 0.3015 - accuracy: 0.89 - 1s 46us/sample - loss: 0.3016 - accuracy: 0.8974 - val_loss: 0.3420 - val_accuracy: 0.8698\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - ETA: 0s - loss: 0.3105 - accuracy: 0.89 - ETA: 0s - loss: 0.2975 - accuracy: 0.89 - ETA: 0s - loss: 0.2943 - accuracy: 0.89 - ETA: 0s - loss: 0.2914 - accuracy: 0.90 - ETA: 0s - loss: 0.2911 - accuracy: 0.90 - ETA: 0s - loss: 0.2903 - accuracy: 0.90 - ETA: 0s - loss: 0.2881 - accuracy: 0.90 - ETA: 0s - loss: 0.2880 - accuracy: 0.90 - ETA: 0s - loss: 0.2856 - accuracy: 0.90 - ETA: 0s - loss: 0.2847 - accuracy: 0.90 - 1s 45us/sample - loss: 0.2837 - accuracy: 0.9036 - val_loss: 0.3310 - val_accuracy: 0.8724\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.91 - ETA: 0s - loss: 0.2642 - accuracy: 0.91 - ETA: 0s - loss: 0.2663 - accuracy: 0.90 - ETA: 0s - loss: 0.2678 - accuracy: 0.90 - ETA: 0s - loss: 0.2642 - accuracy: 0.90 - ETA: 0s - loss: 0.2688 - accuracy: 0.90 - ETA: 0s - loss: 0.2696 - accuracy: 0.90 - ETA: 0s - loss: 0.2692 - accuracy: 0.90 - ETA: 0s - loss: 0.2713 - accuracy: 0.90 - ETA: 0s - loss: 0.2693 - accuracy: 0.90 - 1s 46us/sample - loss: 0.2680 - accuracy: 0.9063 - val_loss: 0.3221 - val_accuracy: 0.8749\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.89 - ETA: 0s - loss: 0.2601 - accuracy: 0.90 - ETA: 0s - loss: 0.2563 - accuracy: 0.91 - ETA: 0s - loss: 0.2545 - accuracy: 0.90 - ETA: 0s - loss: 0.2560 - accuracy: 0.91 - ETA: 0s - loss: 0.2515 - accuracy: 0.91 - ETA: 0s - loss: 0.2558 - accuracy: 0.91 - ETA: 0s - loss: 0.2557 - accuracy: 0.91 - ETA: 0s - loss: 0.2554 - accuracy: 0.91 - ETA: 0s - loss: 0.2544 - accuracy: 0.91 - 1s 44us/sample - loss: 0.2536 - accuracy: 0.9119 - val_loss: 0.3155 - val_accuracy: 0.8754\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.2188 - accuracy: 0.93 - ETA: 0s - loss: 0.2437 - accuracy: 0.91 - ETA: 0s - loss: 0.2433 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2422 - accuracy: 0.91 - ETA: 0s - loss: 0.2429 - accuracy: 0.91 - ETA: 0s - loss: 0.2434 - accuracy: 0.91 - ETA: 0s - loss: 0.2431 - accuracy: 0.91 - ETA: 0s - loss: 0.2416 - accuracy: 0.91 - ETA: 0s - loss: 0.2426 - accuracy: 0.91 - 1s 45us/sample - loss: 0.2416 - accuracy: 0.9156 - val_loss: 0.3076 - val_accuracy: 0.8790\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.93 - ETA: 0s - loss: 0.2282 - accuracy: 0.92 - ETA: 0s - loss: 0.2333 - accuracy: 0.92 - ETA: 0s - loss: 0.2329 - accuracy: 0.92 - ETA: 0s - loss: 0.2377 - accuracy: 0.91 - ETA: 0s - loss: 0.2368 - accuracy: 0.91 - ETA: 0s - loss: 0.2335 - accuracy: 0.92 - ETA: 0s - loss: 0.2305 - accuracy: 0.92 - ETA: 0s - loss: 0.2293 - accuracy: 0.92 - ETA: 0s - loss: 0.2296 - accuracy: 0.92 - 1s 45us/sample - loss: 0.2293 - accuracy: 0.9212 - val_loss: 0.3025 - val_accuracy: 0.8812\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.2373 - accuracy: 0.91 - ETA: 0s - loss: 0.2272 - accuracy: 0.92 - ETA: 0s - loss: 0.2235 - accuracy: 0.92 - ETA: 0s - loss: 0.2214 - accuracy: 0.92 - ETA: 0s - loss: 0.2225 - accuracy: 0.92 - ETA: 0s - loss: 0.2222 - accuracy: 0.92 - ETA: 0s - loss: 0.2210 - accuracy: 0.92 - ETA: 0s - loss: 0.2200 - accuracy: 0.92 - ETA: 0s - loss: 0.2193 - accuracy: 0.92 - ETA: 0s - loss: 0.2180 - accuracy: 0.92 - 1s 45us/sample - loss: 0.2190 - accuracy: 0.9249 - val_loss: 0.2984 - val_accuracy: 0.8822\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.2056 - accuracy: 0.91 - ETA: 0s - loss: 0.2184 - accuracy: 0.92 - ETA: 0s - loss: 0.2189 - accuracy: 0.92 - ETA: 0s - loss: 0.2127 - accuracy: 0.92 - ETA: 0s - loss: 0.2149 - accuracy: 0.92 - ETA: 0s - loss: 0.2109 - accuracy: 0.92 - ETA: 0s - loss: 0.2100 - accuracy: 0.92 - ETA: 0s - loss: 0.2078 - accuracy: 0.92 - ETA: 0s - loss: 0.2084 - accuracy: 0.92 - ETA: 0s - loss: 0.2092 - accuracy: 0.92 - 1s 45us/sample - loss: 0.2088 - accuracy: 0.9287 - val_loss: 0.2948 - val_accuracy: 0.8836\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.91 - ETA: 0s - loss: 0.2047 - accuracy: 0.92 - ETA: 0s - loss: 0.2026 - accuracy: 0.92 - ETA: 0s - loss: 0.1981 - accuracy: 0.93 - ETA: 0s - loss: 0.1994 - accuracy: 0.93 - ETA: 0s - loss: 0.1998 - accuracy: 0.93 - ETA: 0s - loss: 0.1995 - accuracy: 0.93 - ETA: 0s - loss: 0.1989 - accuracy: 0.93 - ETA: 0s - loss: 0.1995 - accuracy: 0.93 - ETA: 0s - loss: 0.1987 - accuracy: 0.93 - 1s 45us/sample - loss: 0.1997 - accuracy: 0.9311 - val_loss: 0.2920 - val_accuracy: 0.8834\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.2027 - accuracy: 0.93 - ETA: 0s - loss: 0.1806 - accuracy: 0.94 - ETA: 0s - loss: 0.1880 - accuracy: 0.93 - ETA: 0s - loss: 0.1875 - accuracy: 0.94 - ETA: 0s - loss: 0.1891 - accuracy: 0.93 - ETA: 0s - loss: 0.1903 - accuracy: 0.93 - ETA: 0s - loss: 0.1907 - accuracy: 0.93 - ETA: 0s - loss: 0.1904 - accuracy: 0.93 - ETA: 0s - loss: 0.1911 - accuracy: 0.93 - ETA: 0s - loss: 0.1909 - accuracy: 0.93 - 1s 45us/sample - loss: 0.1911 - accuracy: 0.9359 - val_loss: 0.2897 - val_accuracy: 0.8843\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1826 - accuracy: 0.92 - ETA: 0s - loss: 0.1864 - accuracy: 0.93 - ETA: 0s - loss: 0.1862 - accuracy: 0.93 - ETA: 0s - loss: 0.1869 - accuracy: 0.93 - ETA: 0s - loss: 0.1879 - accuracy: 0.93 - ETA: 0s - loss: 0.1863 - accuracy: 0.93 - ETA: 0s - loss: 0.1845 - accuracy: 0.93 - ETA: 0s - loss: 0.1841 - accuracy: 0.93 - ETA: 0s - loss: 0.1846 - accuracy: 0.93 - ETA: 0s - loss: 0.1823 - accuracy: 0.93 - 1s 45us/sample - loss: 0.1830 - accuracy: 0.9390 - val_loss: 0.2882 - val_accuracy: 0.8858\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1783 - accuracy: 0.93 - ETA: 0s - loss: 0.1768 - accuracy: 0.93 - ETA: 0s - loss: 0.1732 - accuracy: 0.94 - ETA: 0s - loss: 0.1778 - accuracy: 0.94 - ETA: 0s - loss: 0.1733 - accuracy: 0.94 - ETA: 0s - loss: 0.1725 - accuracy: 0.94 - ETA: 0s - loss: 0.1739 - accuracy: 0.94 - ETA: 0s - loss: 0.1739 - accuracy: 0.94 - ETA: 0s - loss: 0.1750 - accuracy: 0.94 - ETA: 0s - loss: 0.1751 - accuracy: 0.94 - 1s 45us/sample - loss: 0.1754 - accuracy: 0.9431 - val_loss: 0.2868 - val_accuracy: 0.8853\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.94 - ETA: 0s - loss: 0.1754 - accuracy: 0.94 - ETA: 0s - loss: 0.1685 - accuracy: 0.95 - ETA: 0s - loss: 0.1706 - accuracy: 0.94 - ETA: 0s - loss: 0.1694 - accuracy: 0.94 - ETA: 0s - loss: 0.1710 - accuracy: 0.94 - ETA: 0s - loss: 0.1696 - accuracy: 0.94 - ETA: 0s - loss: 0.1673 - accuracy: 0.94 - ETA: 0s - loss: 0.1675 - accuracy: 0.94 - ETA: 0s - loss: 0.1686 - accuracy: 0.94 - 1s 47us/sample - loss: 0.1686 - accuracy: 0.9458 - val_loss: 0.2874 - val_accuracy: 0.8841\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.97 - ETA: 0s - loss: 0.1528 - accuracy: 0.95 - ETA: 0s - loss: 0.1592 - accuracy: 0.95 - ETA: 0s - loss: 0.1616 - accuracy: 0.95 - ETA: 0s - loss: 0.1633 - accuracy: 0.94 - ETA: 0s - loss: 0.1631 - accuracy: 0.94 - ETA: 0s - loss: 0.1638 - accuracy: 0.94 - ETA: 0s - loss: 0.1631 - accuracy: 0.94 - ETA: 0s - loss: 0.1633 - accuracy: 0.94 - ETA: 0s - loss: 0.1612 - accuracy: 0.94 - 1s 46us/sample - loss: 0.1620 - accuracy: 0.9493 - val_loss: 0.2861 - val_accuracy: 0.8859\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.95 - ETA: 0s - loss: 0.1561 - accuracy: 0.95 - ETA: 0s - loss: 0.1553 - accuracy: 0.95 - ETA: 0s - loss: 0.1560 - accuracy: 0.95 - ETA: 0s - loss: 0.1561 - accuracy: 0.95 - ETA: 0s - loss: 0.1553 - accuracy: 0.95 - ETA: 0s - loss: 0.1567 - accuracy: 0.95 - ETA: 0s - loss: 0.1574 - accuracy: 0.95 - ETA: 0s - loss: 0.1554 - accuracy: 0.95 - ETA: 0s - loss: 0.1551 - accuracy: 0.95 - 1s 46us/sample - loss: 0.1553 - accuracy: 0.9518 - val_loss: 0.2856 - val_accuracy: 0.8862\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1595 - accuracy: 0.95 - ETA: 0s - loss: 0.1465 - accuracy: 0.95 - ETA: 0s - loss: 0.1508 - accuracy: 0.95 - ETA: 0s - loss: 0.1524 - accuracy: 0.95 - ETA: 0s - loss: 0.1484 - accuracy: 0.95 - ETA: 0s - loss: 0.1485 - accuracy: 0.95 - ETA: 0s - loss: 0.1479 - accuracy: 0.95 - ETA: 0s - loss: 0.1473 - accuracy: 0.95 - ETA: 0s - loss: 0.1461 - accuracy: 0.95 - ETA: 0s - loss: 0.1490 - accuracy: 0.95 - 1s 45us/sample - loss: 0.1492 - accuracy: 0.9541 - val_loss: 0.2865 - val_accuracy: 0.8869\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.94 - ETA: 0s - loss: 0.1494 - accuracy: 0.95 - ETA: 0s - loss: 0.1421 - accuracy: 0.95 - ETA: 0s - loss: 0.1410 - accuracy: 0.95 - ETA: 0s - loss: 0.1452 - accuracy: 0.95 - ETA: 0s - loss: 0.1455 - accuracy: 0.95 - ETA: 0s - loss: 0.1450 - accuracy: 0.95 - ETA: 0s - loss: 0.1441 - accuracy: 0.95 - ETA: 0s - loss: 0.1459 - accuracy: 0.95 - ETA: 0s - loss: 0.1441 - accuracy: 0.95 - 1s 45us/sample - loss: 0.1435 - accuracy: 0.9564 - val_loss: 0.2881 - val_accuracy: 0.8851\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.97 - ETA: 0s - loss: 0.1297 - accuracy: 0.96 - ETA: 0s - loss: 0.1330 - accuracy: 0.96 - ETA: 0s - loss: 0.1358 - accuracy: 0.96 - ETA: 0s - loss: 0.1359 - accuracy: 0.96 - ETA: 0s - loss: 0.1371 - accuracy: 0.95 - ETA: 0s - loss: 0.1384 - accuracy: 0.95 - ETA: 0s - loss: 0.1385 - accuracy: 0.95 - ETA: 0s - loss: 0.1380 - accuracy: 0.95 - ETA: 0s - loss: 0.1371 - accuracy: 0.95 - 1s 45us/sample - loss: 0.1379 - accuracy: 0.9592 - val_loss: 0.2881 - val_accuracy: 0.8856\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.96 - ETA: 0s - loss: 0.1244 - accuracy: 0.96 - ETA: 0s - loss: 0.1236 - accuracy: 0.96 - ETA: 0s - loss: 0.1247 - accuracy: 0.96 - ETA: 0s - loss: 0.1277 - accuracy: 0.96 - ETA: 0s - loss: 0.1307 - accuracy: 0.96 - ETA: 0s - loss: 0.1301 - accuracy: 0.96 - ETA: 0s - loss: 0.1308 - accuracy: 0.96 - ETA: 0s - loss: 0.1331 - accuracy: 0.96 - ETA: 0s - loss: 0.1321 - accuracy: 0.96 - 1s 45us/sample - loss: 0.1328 - accuracy: 0.9609 - val_loss: 0.2894 - val_accuracy: 0.8872\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.95 - ETA: 0s - loss: 0.1378 - accuracy: 0.95 - ETA: 0s - loss: 0.1368 - accuracy: 0.95 - ETA: 0s - loss: 0.1362 - accuracy: 0.95 - ETA: 0s - loss: 0.1329 - accuracy: 0.96 - ETA: 0s - loss: 0.1298 - accuracy: 0.96 - ETA: 0s - loss: 0.1284 - accuracy: 0.96 - ETA: 0s - loss: 0.1293 - accuracy: 0.96 - ETA: 0s - loss: 0.1291 - accuracy: 0.96 - ETA: 0s - loss: 0.1287 - accuracy: 0.96 - 1s 45us/sample - loss: 0.1278 - accuracy: 0.9619 - val_loss: 0.2904 - val_accuracy: 0.8869\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.97 - ETA: 0s - loss: 0.1289 - accuracy: 0.96 - ETA: 0s - loss: 0.1249 - accuracy: 0.96 - ETA: 0s - loss: 0.1241 - accuracy: 0.96 - ETA: 0s - loss: 0.1230 - accuracy: 0.96 - ETA: 0s - loss: 0.1232 - accuracy: 0.96 - ETA: 0s - loss: 0.1228 - accuracy: 0.96 - ETA: 0s - loss: 0.1239 - accuracy: 0.96 - ETA: 0s - loss: 0.1232 - accuracy: 0.96 - ETA: 0s - loss: 0.1228 - accuracy: 0.96 - 1s 45us/sample - loss: 0.1229 - accuracy: 0.9651 - val_loss: 0.2924 - val_accuracy: 0.8854\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.95 - ETA: 0s - loss: 0.1235 - accuracy: 0.96 - ETA: 0s - loss: 0.1216 - accuracy: 0.96 - ETA: 0s - loss: 0.1242 - accuracy: 0.96 - ETA: 0s - loss: 0.1203 - accuracy: 0.96 - ETA: 0s - loss: 0.1218 - accuracy: 0.96 - ETA: 0s - loss: 0.1207 - accuracy: 0.96 - ETA: 0s - loss: 0.1195 - accuracy: 0.96 - ETA: 0s - loss: 0.1191 - accuracy: 0.96 - ETA: 0s - loss: 0.1187 - accuracy: 0.96 - 1s 45us/sample - loss: 0.1182 - accuracy: 0.9665 - val_loss: 0.2946 - val_accuracy: 0.8855\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1237 - accuracy: 0.96 - ETA: 0s - loss: 0.1141 - accuracy: 0.96 - ETA: 0s - loss: 0.1183 - accuracy: 0.96 - ETA: 0s - loss: 0.1162 - accuracy: 0.96 - ETA: 0s - loss: 0.1138 - accuracy: 0.96 - ETA: 0s - loss: 0.1132 - accuracy: 0.96 - ETA: 0s - loss: 0.1157 - accuracy: 0.96 - ETA: 0s - loss: 0.1156 - accuracy: 0.96 - ETA: 0s - loss: 0.1141 - accuracy: 0.96 - ETA: 0s - loss: 0.1134 - accuracy: 0.96 - 1s 47us/sample - loss: 0.1137 - accuracy: 0.9683 - val_loss: 0.2966 - val_accuracy: 0.8847\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1413 - accuracy: 0.96 - ETA: 0s - loss: 0.1116 - accuracy: 0.97 - ETA: 0s - loss: 0.1107 - accuracy: 0.97 - ETA: 0s - loss: 0.1166 - accuracy: 0.96 - ETA: 0s - loss: 0.1137 - accuracy: 0.96 - ETA: 0s - loss: 0.1145 - accuracy: 0.96 - ETA: 0s - loss: 0.1144 - accuracy: 0.96 - ETA: 0s - loss: 0.1122 - accuracy: 0.96 - ETA: 0s - loss: 0.1111 - accuracy: 0.96 - ETA: 0s - loss: 0.1102 - accuracy: 0.96 - 1s 47us/sample - loss: 0.1097 - accuracy: 0.9695 - val_loss: 0.2988 - val_accuracy: 0.8851\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1122 - accuracy: 0.96 - ETA: 0s - loss: 0.0949 - accuracy: 0.97 - ETA: 0s - loss: 0.0982 - accuracy: 0.97 - ETA: 0s - loss: 0.0991 - accuracy: 0.97 - ETA: 0s - loss: 0.0982 - accuracy: 0.97 - ETA: 0s - loss: 0.0994 - accuracy: 0.97 - ETA: 0s - loss: 0.1002 - accuracy: 0.97 - ETA: 0s - loss: 0.1017 - accuracy: 0.97 - ETA: 0s - loss: 0.1033 - accuracy: 0.97 - ETA: 0s - loss: 0.1054 - accuracy: 0.97 - 1s 45us/sample - loss: 0.1057 - accuracy: 0.9707 - val_loss: 0.3026 - val_accuracy: 0.8830\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.97 - ETA: 0s - loss: 0.1132 - accuracy: 0.96 - ETA: 0s - loss: 0.1135 - accuracy: 0.96 - ETA: 0s - loss: 0.1105 - accuracy: 0.96 - ETA: 0s - loss: 0.1063 - accuracy: 0.97 - ETA: 0s - loss: 0.1032 - accuracy: 0.97 - ETA: 0s - loss: 0.1035 - accuracy: 0.97 - ETA: 0s - loss: 0.1033 - accuracy: 0.97 - ETA: 0s - loss: 0.1025 - accuracy: 0.97 - ETA: 0s - loss: 0.1023 - accuracy: 0.97 - 1s 46us/sample - loss: 0.1015 - accuracy: 0.9727 - val_loss: 0.3047 - val_accuracy: 0.8838\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.97 - ETA: 0s - loss: 0.0984 - accuracy: 0.97 - ETA: 0s - loss: 0.0973 - accuracy: 0.97 - ETA: 0s - loss: 0.0968 - accuracy: 0.97 - ETA: 0s - loss: 0.0980 - accuracy: 0.97 - ETA: 0s - loss: 0.0977 - accuracy: 0.97 - ETA: 0s - loss: 0.0983 - accuracy: 0.97 - ETA: 0s - loss: 0.0979 - accuracy: 0.97 - ETA: 0s - loss: 0.0984 - accuracy: 0.97 - ETA: 0s - loss: 0.0979 - accuracy: 0.97 - 1s 45us/sample - loss: 0.0982 - accuracy: 0.9736 - val_loss: 0.3088 - val_accuracy: 0.8820\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/1 - 1s - loss: 0.3505 - accuracy: 0.8727\n",
      "[0.3269563537120819, 0.87272]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data,  test_labels, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정확도와 손실 그래프 그리기\n",
    "`model.fit()`은 `History`객체를 반환하는데 여기에는 훈련하는 동안 일어난 모든 정보가 `딕셔너리`형태로 저장되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
